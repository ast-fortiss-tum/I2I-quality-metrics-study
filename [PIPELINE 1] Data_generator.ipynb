{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb0be94a",
   "metadata": {},
   "source": [
    "# Data generator\n",
    "This notebook uses the simulated and real datasets to produce the generated images using 3 pix2pix and 3 CycleGAN networks.\n",
    "It aslo produces the segmentation masks using Segformer.\n",
    "\n",
    "## Usage:\n",
    "### task_type='donkey' <- Choose task domain 'donkey' or 'kitti' for lane-keeping or vehicle detection\n",
    "Run all cells\n",
    "\n",
    "## Requirements:\n",
    "-Segformer checkpoints for segmentation: \\\n",
    "    ./[task_type]/content/segmentation_checkpoints/Model_weights.hdf5 \\\n",
    "    \n",
    "    Please refer to specific [task_type] to generate segmentation checkpoint\n",
    "\n",
    "-CycleGAN and pix2pix checkpoints for I2I translation:\\\n",
    "    ./[task_type]/content/gan_checkpoints/cyclegan_checkpoints/1/ \\\n",
    "    ./[task_type]/content/gan_checkpoints/cyclegan_checkpoints/2/ \\\n",
    "    ./[task_type]/content/gan_checkpoints/cyclegan_checkpoints/3/ \\\n",
    "    ./[task_type]/content/gan_checkpoints/pix2pix_checkpoints/1/ \\\n",
    "    ./[task_type]/content/gan_checkpoints/pix2pix_checkpoints/2/ \\\n",
    "    ./[task_type]/content/gan_checkpoints/pix2pix_checkpoints/3/ \n",
    "\n",
    "    Please refer to specific [task_type] to generate GAN checkpoints\n",
    "\n",
    "-Dataset h5 files: \\\n",
    "    KITTI: \\\n",
    "    ./[task_type]/content/datasets/h5_out/bounding_boxes_real.h5\\\n",
    "    ./[task_type]/content/datasets/h5_out/bounding_boxes_sim.h5\\\n",
    "    ./[task_type]/content/datasets/h5_out/raw_image_real.h5\\\n",
    "    ./[task_type]/content/datasets/h5_out/raw_image_sim.h5\\\n",
    "    ./[task_type]/content/datasets/h5_out/segmentation_masks_real.h5\\\n",
    "    ./[task_type]/content/datasets/h5_out/segmentation_masks_sim.h5\\\n",
    "    ./[task_type]/content/datasets/h5_out/semantic_id_list_real.h5\\\n",
    "    ./[task_type]/content/datasets/h5_out/semantic_id_list_sim.h5\\\n",
    "    DONKEY:\\\n",
    "    ./[task_type]/content/datasets/h5_out/gt_real.h5\\\n",
    "    ./[task_type]/content/datasets/h5_out/raw_image_real.h5\\\n",
    "    ./[task_type]/content/datasets/h5_out/raw_image_sim.h5\\\n",
    "    ./[task_type]/content/datasets/h5_out/semantic_id_list_real.h5\\\n",
    "    ./[task_type]/content/datasets/h5_out/semantic_id_list_sim.h5\\\n",
    "\n",
    "    Please refer to specific [task_type] to generate h5 files\n",
    "\n",
    "## Outputs:\n",
    "    ./[task_type]/content/output_plots/[domain_type]/[domain_type] \n",
    "    ./[task_type]/content/output_plots/[domain_type]/[domain_type]_mask \n",
    "    ./[task_type]/content/output_plots/[domain_type]/[domain_type]_additional_mask \n",
    "    ./[task_type]/content/output_plots/[domain_type]/[domain_type]_mask_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c671b3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import tensorflow as tf\n",
    "from tensorflow_examples.models.pix2pix import pix2pix\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import data_generator_utils\n",
    "\n",
    "\n",
    "OUTPUT_CHANNELS = 3\n",
    "BUFFER_SIZE = 1000\n",
    "BATCH_SIZE = 1\n",
    "IMG_WIDTH = 512\n",
    "IMG_HEIGHT = 512\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ffe4bc6-f4cc-4f03-afad-b8fe36223c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sim_real_outputs(\n",
    "    task_type,\n",
    "    segmentation_model,\n",
    "    real_path,\n",
    "    sim_path,\n",
    "    additional_id,\n",
    "    height,\n",
    "    width,\n",
    "    dataset_index_list_test,\n",
    "    test_indexes_gan,\n",
    "    loaded_dictionary_images_real,\n",
    "    loaded_dictionary_images_sim\n",
    "):\n",
    "    for dataset_index in dataset_index_list_test:\n",
    "          print(dataset_index)\n",
    "          counter=0\n",
    "          for i in test_indexes_gan[dataset_index]:\n",
    "                if i%50==0:\n",
    "                    print(i,\" out of \",len(test_indexes_gan[dataset_index]))\n",
    "                counter+=1\n",
    "                input_image_real = np.array(loaded_dictionary_images_real[dataset_index][i])\n",
    "                input_image_real = tf.convert_to_tensor(input_image_real.astype(np.uint8), np.uint8)\n",
    "                input_image_real = tf.reverse(input_image_real, axis=[-1])\n",
    "                \n",
    "                png_image = tf.image.encode_png(input_image_real)\n",
    "                os.makedirs('./'+task_type+'/content/output_plots/real/'+real_path+'/', exist_ok=True)\n",
    "                with open('./'+task_type+'/content/output_plots/real/'+real_path+'/'+str(dataset_index)+'_'+str(i)+'.png', 'wb') as f:\n",
    "                  f.write(png_image.numpy())\n",
    "                input_image_real = tf.cast(input_image_real, np.float16)\n",
    "                input_image_real = tf.transpose(input_image_real, (2, 0, 1))\n",
    "        \n",
    "                input_image_sim = np.array(loaded_dictionary_images_sim[dataset_index][i])\n",
    "                input_image_sim_for_pix2pix=input_image_sim\n",
    "                input_image_sim = tf.convert_to_tensor(input_image_sim.astype(np.uint8), np.uint8)\n",
    "                input_image_sim = tf.reverse(input_image_sim, axis=[-1])\n",
    "                png_image = tf.image.encode_png(tf.cast(input_image_sim,np.uint8))\n",
    "                os.makedirs('./'+task_type+'/content/output_plots/sim/'+sim_path+'/', exist_ok=True)\n",
    "                with open('./'+task_type+'/content/output_plots/sim/'+sim_path+'/'+str(dataset_index)+'_'+str(i)+'.png', 'wb') as f:\n",
    "                  f.write(png_image.numpy())\n",
    "                input_image_sim = tf.cast(input_image_sim, np.float16)\n",
    "                input_image_sim = tf.transpose(input_image_sim, (2, 0, 1))\n",
    "                \n",
    "                colored_mask = np.zeros_like(input_image_sim_for_pix2pix)\n",
    "                input_image_real=tf.expand_dims(input_image_real, 0)\n",
    "                \n",
    "                pred_masks =segmentation_model.predict(input_image_real).logits\n",
    "                created_mask=data_generator_utils.cast_to_int32(data_generator_utils.create_mask(pred_masks))\n",
    "                created_mask_real = tf.image.resize(created_mask, [height,width],\n",
    "                                  method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "                created_mask_real = tf.cast(created_mask_real, np.uint8)\n",
    "\n",
    "                input_image_sim=tf.expand_dims(input_image_sim, 0)\n",
    "                pred_masks =segmentation_model.predict(input_image_sim).logits\n",
    "                created_mask=data_generator_utils.cast_to_int32(data_generator_utils.create_mask(pred_masks))\n",
    "                created_mask_sim = tf.image.resize(created_mask, [height,width],\n",
    "                                  method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "                created_mask_sim = tf.cast(created_mask_sim, np.uint8)\n",
    "\n",
    "                if task_type==\"kitti\":\n",
    "                    vectorized_map_additional=np.vectorize(data_generator_utils.map_values_car)\n",
    "                    \n",
    "                elif task_type==\"donkey\":\n",
    "                    vectorized_map_additional=np.vectorize(data_generator_utils.map_values_donkey)\n",
    "\n",
    "                real_mask=vectorized_map_additional(created_mask_real)\n",
    "                sim_mask=vectorized_map_additional(created_mask_sim)\n",
    "              \n",
    "                colored_mask[:, :, 0] = np.vectorize(data_generator_utils.map_to_b)(tf.squeeze(created_mask_real))\n",
    "                colored_mask[:, :, 1] = np.vectorize(data_generator_utils.map_to_g)(tf.squeeze(created_mask_real))\n",
    "                colored_mask[:, :, 2] = np.vectorize(data_generator_utils.map_to_r)(tf.squeeze(created_mask_real))\n",
    "                png_image = tf.image.encode_png(colored_mask)\n",
    "                os.makedirs('./'+task_type+'/content/output_plots/real/'+real_path+'_mask/', exist_ok=True)\n",
    "                with open('./'+task_type+'/content/output_plots/real/'+real_path+'_mask/'+str(dataset_index)+'_'+str(i)+'.png', 'wb') as f:\n",
    "                  f.write(png_image.numpy())\n",
    "\n",
    "                if task_type==\"donkey\":\n",
    "                    binary_mask_real1 = (real_mask == 1).astype(np.uint8)\n",
    "                    binary_mask_real2 = (real_mask == 2).astype(np.uint8)\n",
    "                    binary_mask_real = binary_mask_real1+binary_mask_real2\n",
    "                else:\n",
    "                    binary_mask_real = (real_mask == additional_id).astype(np.uint8)\n",
    "\n",
    "                colored_mask[:, :, 0] = np.vectorize(data_generator_utils.map_to_b)(tf.squeeze(binary_mask_real))\n",
    "                colored_mask[:, :, 1] = np.vectorize(data_generator_utils.map_to_g)(tf.squeeze(binary_mask_real))\n",
    "                colored_mask[:, :, 2] = np.vectorize(data_generator_utils.map_to_r)(tf.squeeze(binary_mask_real))\n",
    "                png_image = tf.image.encode_png(colored_mask)\n",
    "                os.makedirs('./'+task_type+'/content/output_plots/real/'+real_path+'_additional_mask/', exist_ok=True)\n",
    "                with open('./'+task_type+'/content/output_plots/real/'+real_path+'_additional_mask/'+str(dataset_index)+'_'+str(i)+'.png', 'wb') as f:\n",
    "                  f.write(png_image.numpy())\n",
    "\n",
    "                colored_mask[:, :, 0] = np.vectorize(data_generator_utils.map_to_b)(tf.squeeze(created_mask_sim))\n",
    "                colored_mask[:, :, 1] = np.vectorize(data_generator_utils.map_to_g)(tf.squeeze(created_mask_sim))\n",
    "                colored_mask[:, :, 2] = np.vectorize(data_generator_utils.map_to_r)(tf.squeeze(created_mask_sim))\n",
    "                png_image = tf.image.encode_png(colored_mask)\n",
    "                os.makedirs('./'+task_type+'/content/output_plots/sim/'+sim_path+'_mask/', exist_ok=True)\n",
    "                with open('./'+task_type+'/content/output_plots/sim/'+sim_path+'_mask/'+str(dataset_index)+'_'+str(i)+'.png', 'wb') as f:\n",
    "                  f.write(png_image.numpy())\n",
    "              \n",
    "                if task_type==\"donkey\":\n",
    "                    binary_mask_sim1 = (sim_mask == 1).astype(np.uint8)\n",
    "                    binary_mask_sim2 = (sim_mask == 2).astype(np.uint8)\n",
    "                    binary_mask_sim = binary_mask_sim1+binary_mask_sim2\n",
    "                else:\n",
    "                    binary_mask_sim = (sim_mask == additional_id).astype(np.uint8)\n",
    "                colored_mask[:, :, 0] = np.vectorize(data_generator_utils.map_to_b)(tf.squeeze(binary_mask_sim))\n",
    "                colored_mask[:, :, 1] = np.vectorize(data_generator_utils.map_to_g)(tf.squeeze(binary_mask_sim))\n",
    "                colored_mask[:, :, 2] = np.vectorize(data_generator_utils.map_to_r)(tf.squeeze(binary_mask_sim))\n",
    "                png_image = tf.image.encode_png(colored_mask)\n",
    "                os.makedirs('./'+task_type+'/content/output_plots/sim/'+sim_path+'_additional_mask/', exist_ok=True)\n",
    "                with open('./'+task_type+'/content/output_plots/sim/'+sim_path+'_additional_mask/'+str(dataset_index)+'_'+str(i)+'.png', 'wb') as f:\n",
    "                  f.write(png_image.numpy())\n",
    "                \n",
    "                real_mask = np.array(real_mask)\n",
    "                real_mask = tf.convert_to_tensor(real_mask.astype(np.float16), np.float16)\n",
    "    \n",
    "                sim_mask = np.array(sim_mask)\n",
    "                sim_mask = tf.convert_to_tensor(sim_mask.astype(np.float16), np.float16)\n",
    "\n",
    "                binary_mask_real = np.array(binary_mask_real)\n",
    "                binary_mask_real = tf.convert_to_tensor(binary_mask_real.astype(np.float16), np.float16)\n",
    "\n",
    "                binary_mask_sim = np.array(binary_mask_sim)\n",
    "                binary_mask_sim = tf.convert_to_tensor(binary_mask_sim.astype(np.float16), np.float16)\n",
    "    \n",
    "                real_mask=data_generator_utils.cast_to_int32(real_mask)\n",
    "                sim_mask=data_generator_utils.cast_to_int32(sim_mask)\n",
    "                error_sim_real=abs(sim_mask-real_mask)\n",
    "                error_sim_real_car=abs(binary_mask_sim-binary_mask_real)\n",
    "\n",
    "                error_data_sim = {\n",
    "                    \"sim_real\": np.sum(error_sim_real) / (height * width),\n",
    "                    \"additional\": np.sum(error_sim_real_car) / (height * width)\n",
    "                }\n",
    "\n",
    "                output_folder = './'+task_type+'/content/output_plots/sim/'+sim_path+'_mask_error/'\n",
    "                os.makedirs(output_folder, exist_ok=True)\n",
    "                output_path = output_folder + str(dataset_index) + \"_\" + str(i) +\".json\"\n",
    "                with open(output_path, \"w\") as json_file:\n",
    "                    json.dump(error_data_sim, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "822d40f2-6542-43eb-8400-0b2e21e2a230",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cyclegan_outputs(\n",
    "    task_type,\n",
    "    segmentation_model,\n",
    "    generator_cyclegan,\n",
    "    cyclegan_name,\n",
    "    additional_id,\n",
    "    height,\n",
    "    width,\n",
    "    dataset_index_list_test,\n",
    "    test_indexes_gan,\n",
    "    loaded_dictionary_images_real,\n",
    "    loaded_dictionary_images_sim\n",
    "):\n",
    "    for dataset_index in dataset_index_list_test:\n",
    "          counter=0\n",
    "          for i in test_indexes_gan[dataset_index]:\n",
    "                if i%50==0:\n",
    "                    print(i,\" out of \",len(test_indexes_gan[dataset_index]))\n",
    "                counter+=1\n",
    "                input_image_real = np.array(loaded_dictionary_images_real[dataset_index][i])\n",
    "                input_image_real = tf.convert_to_tensor(input_image_real.astype(np.uint8), np.uint8)\n",
    "                input_image_real = tf.reverse(input_image_real, axis=[-1])\n",
    "                \n",
    "\n",
    "                input_image_real = tf.cast(input_image_real, np.float16)\n",
    "                input_image_real = tf.transpose(input_image_real, (2, 0, 1))\n",
    "        \n",
    "                input_image_sim = np.array(loaded_dictionary_images_sim[dataset_index][i])\n",
    "                input_image_sim_for_pix2pix=input_image_sim\n",
    "                input_image_sim = tf.convert_to_tensor(input_image_sim.astype(np.uint8), np.uint8)\n",
    "                input_image_sim = tf.reverse(input_image_sim, axis=[-1])\n",
    "                input_image_sim = tf.cast(input_image_sim, np.float16)\n",
    "                input_image_sim = tf.transpose(input_image_sim, (2, 0, 1))\n",
    "                \n",
    "                vectorized_map_car=np.vectorize(data_generator_utils.map_values_car)\n",
    "        \n",
    "                input_image_sim_for_cyclegan=tf.expand_dims(input_image_sim, 0)\n",
    "                input_image_sim_for_cyclegan={'pixel_values': input_image_sim_for_cyclegan }\n",
    "              \n",
    "                fake_real_cycle = data_generator_utils.generate_images_cyclegan(generator_cyclegan, input_image_sim_for_cyclegan,False,height,width)\n",
    "                fake_real_cycle = tf.image.resize(fake_real_cycle, [height,width],\n",
    "                                  method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "                fake_real_cycle=fake_real_cycle * (255/2) + (255/2)\n",
    "                fake_real_cycle = tf.cast(fake_real_cycle, np.uint8)\n",
    "                png_image = tf.image.encode_png(fake_real_cycle)\n",
    "                os.makedirs('./'+task_type+'/content/output_plots/cyclegan/'+cyclegan_name+'/', exist_ok=True)\n",
    "                with open('./'+task_type+'/content/output_plots/cyclegan/'+cyclegan_name+'/'+str(dataset_index)+'_'+str(i)+'.png', 'wb') as f:\n",
    "                  f.write(png_image.numpy())\n",
    "                fake_real_cycle = tf.cast(fake_real_cycle, np.float16)\n",
    "                fake_real_cycle = tf.transpose(fake_real_cycle, (2, 0, 1))\n",
    "              \n",
    "                input_image_real=tf.expand_dims(input_image_real, 0)\n",
    "                pred_masks =segmentation_model.predict(input_image_real).logits\n",
    "                created_mask=data_generator_utils.cast_to_int32(data_generator_utils.create_mask(pred_masks))\n",
    "                created_mask_real = tf.image.resize(created_mask, [height,width],\n",
    "                                  method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "                created_mask_real = tf.cast(created_mask_real, np.uint8)\n",
    "                real_label_value =vectorized_map_car(created_mask_real)\n",
    "                binary_mask_real = (real_label_value == additional_id).astype(np.uint8)\n",
    "                if task_type == \"donkey\":\n",
    "                    binary_mask_real = binary_mask_real + (real_label_value == 2).astype(np.uint8)\n",
    "              \n",
    "                fake_real_cycle=tf.expand_dims(fake_real_cycle, 0)\n",
    "                pred_masks =segmentation_model.predict(fake_real_cycle).logits\n",
    "                created_mask=data_generator_utils.cast_to_int32(data_generator_utils.create_mask(pred_masks))\n",
    "                created_mask_cyclegan = tf.image.resize(created_mask, [height,width],\n",
    "                                  method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "                created_mask_cyclegan = tf.cast(created_mask_cyclegan, np.uint8)\n",
    "                cyclegan_label_value =vectorized_map_car(created_mask_cyclegan)\n",
    "                \n",
    "                binary_mask_cyclegan = (cyclegan_label_value == additional_id).astype(np.uint8)\n",
    "                if task_type == \"donkey\":\n",
    "                    binary_mask_cyclegan = binary_mask_cyclegan + (cyclegan_label_value == 2).astype(np.uint8)\n",
    "                colored_mask = np.zeros_like(input_image_sim_for_pix2pix)\n",
    "                colored_mask[:, :, 0] = np.vectorize(data_generator_utils.map_to_b)(tf.squeeze(binary_mask_cyclegan))\n",
    "                colored_mask[:, :, 1] = np.vectorize(data_generator_utils.map_to_g)(tf.squeeze(binary_mask_cyclegan))\n",
    "                colored_mask[:, :, 2] = np.vectorize(data_generator_utils.map_to_r)(tf.squeeze(binary_mask_cyclegan))\n",
    "                png_image = tf.image.encode_png(colored_mask)\n",
    "                os.makedirs('./'+task_type+'/content/output_plots/cyclegan/'+cyclegan_name+'_additional_mask/', exist_ok=True)\n",
    "                with open('./'+task_type+'/content/output_plots/cyclegan/'+cyclegan_name+'_additional_mask/'+str(dataset_index)+'_'+str(i)+'.png', 'wb') as f:\n",
    "                  f.write(png_image.numpy())\n",
    "\n",
    "                colored_mask = np.zeros_like(input_image_sim_for_pix2pix)\n",
    "                colored_mask[:, :, 0] = np.vectorize(data_generator_utils.map_to_b)(tf.squeeze(created_mask_cyclegan))\n",
    "                colored_mask[:, :, 1] = np.vectorize(data_generator_utils.map_to_g)(tf.squeeze(created_mask_cyclegan))\n",
    "                colored_mask[:, :, 2] = np.vectorize(data_generator_utils.map_to_r)(tf.squeeze(created_mask_cyclegan))\n",
    "                png_image = tf.image.encode_png(colored_mask)\n",
    "                os.makedirs('./'+task_type+'/content/output_plots/cyclegan/'+cyclegan_name+'_mask/', exist_ok=True)\n",
    "                with open('./'+task_type+'/content/output_plots/cyclegan/'+cyclegan_name+'_mask/'+str(dataset_index)+'_'+str(i)+'.png', 'wb') as f:\n",
    "                  f.write(png_image.numpy())\n",
    "                \n",
    "                input_image_sim=tf.expand_dims(input_image_sim, 0)\n",
    "                pred_masks =segmentation_model.predict(input_image_sim).logits\n",
    "                created_mask=data_generator_utils.cast_to_int32(data_generator_utils.create_mask(pred_masks))\n",
    "                created_mask_sim = tf.image.resize(created_mask, [height,width],\n",
    "                                  method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "                created_mask_sim = tf.cast(created_mask_sim, np.uint8)\n",
    "\n",
    "                if task_type==\"kitti\":\n",
    "                    vectorized_map=np.vectorize(data_generator_utils.map_values_kitti)\n",
    "                    \n",
    "                elif task_type==\"donkey\":\n",
    "                    vectorized_map=np.vectorize(data_generator_utils.map_values_donkey)\n",
    "                \n",
    "                created_mask_cyclegan=data_generator_utils.cast_to_int32(created_mask_cyclegan)\n",
    "                created_mask_cyclegan=np.squeeze(created_mask_cyclegan)\n",
    "\n",
    "                binary_mask_cyclegan=data_generator_utils.cast_to_int32(binary_mask_cyclegan)\n",
    "                binary_mask_cyclegan=np.squeeze(binary_mask_cyclegan)\n",
    "\n",
    "                binary_mask_real=data_generator_utils.cast_to_int32(binary_mask_real)\n",
    "                binary_mask_real=np.squeeze(binary_mask_real)\n",
    "\n",
    "                created_mask_real=data_generator_utils.cast_to_int32(created_mask_real)\n",
    "                created_mask_real=np.squeeze(created_mask_real)\n",
    "                created_mask_sim=data_generator_utils.cast_to_int32(created_mask_sim)\n",
    "                created_mask_sim=np.squeeze(created_mask_sim)\n",
    "\n",
    "                error_sim_real=abs(created_mask_sim-created_mask_real)\n",
    "                error_cyclegan_sim=abs(created_mask_cyclegan-created_mask_sim)\n",
    "                error_cyclegan_real=abs(created_mask_cyclegan-created_mask_real)\n",
    "                error_cyclegan_car=abs(binary_mask_cyclegan-binary_mask_real)\n",
    "    \n",
    "                error_data_cyclegan = {\n",
    "                    \"sim_real\": np.sum(error_sim_real) / (height * width),\n",
    "                    \"sim\": np.sum(error_cyclegan_sim) / (height * width),\n",
    "                    \"real\": np.sum(error_cyclegan_real) / (height * width),\n",
    "                    \"additional\": np.sum(error_cyclegan_car) / (height * width)\n",
    "                }\n",
    "\n",
    "                output_folder = './'+task_type+'/content/output_plots/cyclegan/'+cyclegan_name+'_mask_error/'\n",
    "                os.makedirs(output_folder, exist_ok=True)\n",
    "                output_path = output_folder + str(dataset_index) + \"_\" + str(i) +\".json\"\n",
    "                with open(output_path, \"w\") as json_file:\n",
    "                    json.dump(error_data_cyclegan, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d01434ea-e671-4243-b4d4-57a5b9509214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pix2pix_mask_outputs(task_type,limit,segmentation_model,two_classes,generator_pix2pix_mask,pix2pix_mask_name,additional_id,height,width,input_domain,pix2pix_mask_type,dataset_index_list_test,test_indexes_gan,loaded_dictionary_images_real,loaded_dictionary_images_sim,loaded_semantic_id_real,loaded_semantic_id_sim):\n",
    "    for dataset_index in dataset_index_list_test:\n",
    "          print(dataset_index)\n",
    "          counter=0\n",
    "          for i in test_indexes_gan[dataset_index]:\n",
    "                if i%50==0:\n",
    "                    print(i,\" out of \",len(test_indexes_gan[dataset_index]))\n",
    "                counter+=1\n",
    "                input_image_real = np.array(loaded_dictionary_images_real[dataset_index][i])\n",
    "                input_image_real = tf.convert_to_tensor(input_image_real.astype(np.uint8), np.uint8)\n",
    "                input_image_real = tf.reverse(input_image_real, axis=[-1])\n",
    "                \n",
    "                \n",
    "                input_image_real = tf.cast(input_image_real, np.float16)\n",
    "                input_image_real = tf.transpose(input_image_real, (2, 0, 1))\n",
    "        \n",
    "                input_image_sim = np.array(loaded_dictionary_images_sim[dataset_index][i])\n",
    "                input_image_sim_for_pix2pix=input_image_sim\n",
    "                input_image_sim = tf.convert_to_tensor(input_image_sim.astype(np.uint8), np.uint8)\n",
    "                input_image_sim = tf.reverse(input_image_sim, axis=[-1])\n",
    "                \n",
    "                input_image_sim = tf.cast(input_image_sim, np.float16)\n",
    "              \n",
    "                input_image_sim = tf.transpose(input_image_sim, (2, 0, 1))\n",
    "                \n",
    "                vectorized_map_car=np.vectorize(data_generator_utils.map_values_car)\n",
    "        \n",
    "                colored_mask = np.zeros_like(input_image_sim_for_pix2pix)\n",
    "                colored_mask2 = np.zeros_like(input_image_sim_for_pix2pix)\n",
    "\n",
    "                input_image_real=tf.expand_dims(input_image_real, 0)\n",
    "                pred_masks =segmentation_model.predict(input_image_real).logits\n",
    "                created_mask=data_generator_utils.cast_to_int32(data_generator_utils.create_mask(pred_masks))\n",
    "                created_mask_real = tf.image.resize(created_mask, [height,width],\n",
    "                                  method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "                created_mask_real = tf.cast(created_mask_real, np.uint8)\n",
    "\n",
    "                real_label_value =vectorized_map_car(created_mask_real)\n",
    "                binary_mask_real = (real_label_value == additional_id).astype(np.uint8)\n",
    "                if task_type == \"donkey\":\n",
    "                    binary_mask_real = binary_mask_real + (real_label_value == 2).astype(np.uint8)\n",
    "\n",
    "                input_image_sim=tf.expand_dims(input_image_sim, 0)\n",
    "                pred_masks =segmentation_model.predict(input_image_sim).logits\n",
    "                created_mask=data_generator_utils.cast_to_int32(data_generator_utils.create_mask(pred_masks))\n",
    "                created_mask_sim = tf.image.resize(created_mask, [height,width],\n",
    "                                  method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "                created_mask_sim = tf.cast(created_mask_sim, np.uint8)\n",
    "\n",
    "                if pix2pix_mask_type==\"manual\": \n",
    "                    if (input_domain==\"real\"):\n",
    "                        mask_in=loaded_semantic_id_real[dataset_index][i]\n",
    "                    else:\n",
    "                        mask_in=loaded_semantic_id_sim[dataset_index][i]\n",
    "                    \n",
    "                else:\n",
    "                    if (input_domain==\"real\"):\n",
    "                        mask_in=created_mask_real[:,:,0]\n",
    "                    else:\n",
    "                        mask_in=created_mask_sim[:,:,0]\n",
    "              \n",
    "              \n",
    "                if (input_domain==\"real\"):\n",
    "                    colored_mask[:, :, 0] = np.vectorize(data_generator_utils.map_to_b)(mask_in)\n",
    "                    colored_mask[:, :, 1] = np.vectorize(data_generator_utils.map_to_g)(mask_in)\n",
    "                    colored_mask[:, :, 2] = np.vectorize(data_generator_utils.map_to_r)(mask_in)\n",
    "                    input_image_real_for_pix2pix_mask=colored_mask\n",
    "                    input_image_real_for_pix2pix_mask = tf.convert_to_tensor(input_image_real_for_pix2pix_mask.astype(np.float16), np.float16)\n",
    "                    input_image_real_for_pix2pix_mask = tf.reverse(input_image_real_for_pix2pix_mask, axis=[-1])\n",
    "                    input_image_real_for_pix2pix_mask = tf.image.resize(input_image_real_for_pix2pix_mask, [512,512],\n",
    "                                              method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "                    input_image_real_for_pix2pix_mask=tf.expand_dims(input_image_real_for_pix2pix_mask, 0)\n",
    "                    input_image_real_for_pix2pix_mask = (input_image_real_for_pix2pix_mask / 127.5) - 1\n",
    "                    fake_real_pix2pix_mask_real = data_generator_utils.generate_images_pix2pix(generator_pix2pix_mask, input_image_real_for_pix2pix_mask,False)\n",
    "                    fake_real_pix2pix_mask_real = tf.image.resize(fake_real_pix2pix_mask_real, [height,width],\n",
    "                                      method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "                    fake_real_pix2pix_mask_real=fake_real_pix2pix_mask_real * (255/2) + (255/2)\n",
    "                    fake_real_pix2pix_mask_real = tf.cast(fake_real_pix2pix_mask_real, np.uint8)\n",
    "                    png_image = tf.image.encode_png(fake_real_pix2pix_mask_real)\n",
    "                    os.makedirs('./'+task_type+'/content/output_plots/pix2pix_mask_'+pix2pix_mask_type+'/'+pix2pix_mask_name+'_real/', exist_ok=True)\n",
    "                    with open('./'+task_type+'/content/output_plots/pix2pix_mask_'+pix2pix_mask_type+'/'+pix2pix_mask_name+'_real/'+str(dataset_index)+'_'+str(i)+'.png', 'wb') as f:\n",
    "                        f.write(png_image.numpy())\n",
    "                    fake_real_pix2pix_mask_real = tf.cast(fake_real_pix2pix_mask_real, np.float16)\n",
    "                    fake_real_pix2pix_mask_real = tf.transpose(fake_real_pix2pix_mask_real, (2, 0, 1))\n",
    "\n",
    "\n",
    "                    fake_real_pix2pix_mask_real=tf.expand_dims(fake_real_pix2pix_mask_real, 0)\n",
    "                    pred_masks = segmentation_model.predict(fake_real_pix2pix_mask_real).logits\n",
    "                    created_mask=data_generator_utils.cast_to_int32(data_generator_utils.create_mask(pred_masks))\n",
    "                    created_mask_pix2pix_mask_real = tf.image.resize(created_mask, [height,width],\n",
    "                                      method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "\n",
    "                    pix2pix_label_value=created_mask_pix2pix_mask_real\n",
    "                    pix2pix_label_value =vectorized_map_car(pix2pix_label_value)\n",
    "\n",
    "                    \n",
    "                    \n",
    "                    binary_mask_pix2pix = (pix2pix_label_value == additional_id).astype(np.uint8)\n",
    "                    if task_type == \"donkey\":\n",
    "                        binary_mask_pix2pix = binary_mask_pix2pix + (pix2pix_label_value == 2).astype(np.uint8)\n",
    "                    colored_mask = np.zeros_like(input_image_sim_for_pix2pix)\n",
    "                    colored_mask[:, :, 0] = np.vectorize(data_generator_utils.map_to_b)(tf.squeeze(binary_mask_pix2pix))\n",
    "                    colored_mask[:, :, 1] = np.vectorize(data_generator_utils.map_to_g)(tf.squeeze(binary_mask_pix2pix))\n",
    "                    colored_mask[:, :, 2] = np.vectorize(data_generator_utils.map_to_r)(tf.squeeze(binary_mask_pix2pix))\n",
    "                    png_image = tf.image.encode_png(colored_mask)\n",
    "                    os.makedirs('./'+task_type+'/content/output_plots/pix2pix_mask_'+pix2pix_mask_type+'/'+pix2pix_mask_name+'_real_additional_mask/', exist_ok=True)\n",
    "                    with open('./'+task_type+'/content/output_plots/pix2pix_mask_'+pix2pix_mask_type+'/'+pix2pix_mask_name+'_real_additional_mask/'+str(dataset_index)+'_'+str(i)+'.png', 'wb') as f:\n",
    "                      f.write(png_image.numpy())\n",
    "                    \n",
    "                    created_mask_pix2pix_mask_real = tf.cast(created_mask_pix2pix_mask_real, np.uint8)\n",
    "                    colored_mask[:, :, 0] = np.vectorize(data_generator_utils.map_to_b)(tf.squeeze(created_mask_pix2pix_mask_real))\n",
    "                    colored_mask[:, :, 1] = np.vectorize(data_generator_utils.map_to_g)(tf.squeeze(created_mask_pix2pix_mask_real))\n",
    "                    colored_mask[:, :, 2] = np.vectorize(data_generator_utils.map_to_r)(tf.squeeze(created_mask_pix2pix_mask_real))\n",
    "                    png_image = tf.image.encode_png(colored_mask)\n",
    "                    os.makedirs('./'+task_type+'/content/output_plots/pix2pix_mask_'+pix2pix_mask_type+'/'+pix2pix_mask_name+'_real_mask/', exist_ok=True)\n",
    "                    with open('./'+task_type+'/content/output_plots/pix2pix_mask_'+pix2pix_mask_type+'/'+pix2pix_mask_name+'_real_mask/'+str(dataset_index)+'_'+str(i)+'.png', 'wb') as f:\n",
    "                        f.write(png_image.numpy())\n",
    "                    created_mask_pix2pix_mask_real=data_generator_utils.cast_to_int32(created_mask_pix2pix_mask_real)\n",
    "                    created_mask_pix2pix_mask_real=np.squeeze(created_mask_pix2pix_mask_real)\n",
    "                    \n",
    "\n",
    "              \n",
    "                else:\n",
    "                    colored_mask2[:, :, 0] = np.vectorize(data_generator_utils.map_to_b)(mask_in)\n",
    "                    colored_mask2[:, :, 1] = np.vectorize(data_generator_utils.map_to_g)(mask_in)\n",
    "                    colored_mask2[:, :, 2] = np.vectorize(data_generator_utils.map_to_r)(mask_in)\n",
    "                    input_image_sim_for_pix2pix_mask=colored_mask2\n",
    "                    input_image_sim_for_pix2pix_mask = tf.convert_to_tensor(input_image_sim_for_pix2pix_mask.astype(np.float16), np.float16)\n",
    "                    input_image_sim_for_pix2pix_mask = tf.reverse(input_image_sim_for_pix2pix_mask, axis=[-1])\n",
    "                    input_image_sim_for_pix2pix_mask = tf.image.resize(input_image_sim_for_pix2pix_mask, [512,512],\n",
    "                                              method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "                    input_image_sim_for_pix2pix_mask=tf.expand_dims(input_image_sim_for_pix2pix_mask, 0)\n",
    "                    input_image_sim_for_pix2pix_mask = (input_image_sim_for_pix2pix_mask / 127.5) - 1  \n",
    "                    fake_real_pix2pix_mask_sim = data_generator_utils.generate_images_pix2pix(generator_pix2pix_mask, input_image_sim_for_pix2pix_mask,False)\n",
    "                    fake_real_pix2pix_mask_sim = tf.image.resize(fake_real_pix2pix_mask_sim, [height,width],\n",
    "                                      method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "                    fake_real_pix2pix_mask_sim=fake_real_pix2pix_mask_sim * (255/2) + (255/2)\n",
    "                    fake_real_pix2pix_mask_sim = tf.cast(fake_real_pix2pix_mask_sim, np.uint8)\n",
    "                    png_image = tf.image.encode_png(fake_real_pix2pix_mask_sim)\n",
    "                    os.makedirs('./'+task_type+'/content/output_plots/pix2pix_mask_'+pix2pix_mask_type+'/'+pix2pix_mask_name+'_sim/', exist_ok=True)\n",
    "                    with open('./'+task_type+'/content/output_plots/pix2pix_mask_'+pix2pix_mask_type+'/'+pix2pix_mask_name+'_sim/'+str(dataset_index)+'_'+str(i)+'.png', 'wb') as f:\n",
    "                      f.write(png_image.numpy())\n",
    "                    fake_real_pix2pix_mask_sim = tf.cast(fake_real_pix2pix_mask_sim, np.float16)\n",
    "                    fake_real_pix2pix_mask_sim = tf.transpose(fake_real_pix2pix_mask_sim, (2, 0, 1))\n",
    "\n",
    "                    fake_real_pix2pix_mask_sim=tf.expand_dims(fake_real_pix2pix_mask_sim, 0)\n",
    "                    pred_masks = segmentation_model.predict(fake_real_pix2pix_mask_sim).logits\n",
    "                    created_mask=data_generator_utils.cast_to_int32(data_generator_utils.create_mask(pred_masks))\n",
    "                    created_mask_pix2pix_mask_sim = tf.image.resize(created_mask, [height,width],\n",
    "                                      method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "                    \n",
    "                    \n",
    "                    pix2pix_label_value=created_mask_pix2pix_mask_sim\n",
    "                    pix2pix_label_value =vectorized_map_car(pix2pix_label_value)\n",
    "                    binary_mask_pix2pix = (pix2pix_label_value == additional_id).astype(np.uint8)\n",
    "                    if task_type == \"donkey\":\n",
    "                        binary_mask_pix2pix = binary_mask_pix2pix + (pix2pix_label_value == 2).astype(np.uint8)\n",
    "                    colored_mask = np.zeros_like(input_image_sim_for_pix2pix)\n",
    "                    colored_mask[:, :, 0] = np.vectorize(data_generator_utils.map_to_b)(tf.squeeze(binary_mask_pix2pix))\n",
    "                    colored_mask[:, :, 1] = np.vectorize(data_generator_utils.map_to_g)(tf.squeeze(binary_mask_pix2pix))\n",
    "                    colored_mask[:, :, 2] = np.vectorize(data_generator_utils.map_to_r)(tf.squeeze(binary_mask_pix2pix))\n",
    "                    png_image = tf.image.encode_png(colored_mask)\n",
    "                    os.makedirs('./'+task_type+'/content/output_plots/pix2pix_mask_'+pix2pix_mask_type+'/'+pix2pix_mask_name+'_sim_additional_mask/', exist_ok=True)\n",
    "                    with open('./'+task_type+'/content/output_plots/pix2pix_mask_'+pix2pix_mask_type+'/'+pix2pix_mask_name+'_sim_additional_mask/'+str(dataset_index)+'_'+str(i)+'.png', 'wb') as f:\n",
    "                      f.write(png_image.numpy())\n",
    "                    \n",
    "                    created_mask_pix2pix_mask_sim = tf.cast(created_mask_pix2pix_mask_sim, np.uint8)\n",
    "                    colored_mask[:, :, 0] = np.vectorize(data_generator_utils.map_to_b)(tf.squeeze(created_mask_pix2pix_mask_sim))\n",
    "                    colored_mask[:, :, 1] = np.vectorize(data_generator_utils.map_to_g)(tf.squeeze(created_mask_pix2pix_mask_sim))\n",
    "                    colored_mask[:, :, 2] = np.vectorize(data_generator_utils.map_to_r)(tf.squeeze(created_mask_pix2pix_mask_sim))\n",
    "                    png_image = tf.image.encode_png(colored_mask)\n",
    "                    os.makedirs('./'+task_type+'/content/output_plots/pix2pix_mask_'+pix2pix_mask_type+'/'+pix2pix_mask_name+'_sim_mask/', exist_ok=True)\n",
    "                    with open('./'+task_type+'/content/output_plots/pix2pix_mask_'+pix2pix_mask_type+'/'+pix2pix_mask_name+'_sim_mask/'+str(dataset_index)+'_'+str(i)+'.png', 'wb') as f:\n",
    "                      f.write(png_image.numpy())\n",
    "                    created_mask_pix2pix_mask_sim=data_generator_utils.cast_to_int32(created_mask_pix2pix_mask_sim)\n",
    "                    created_mask_pix2pix_mask_sim=np.squeeze(created_mask_pix2pix_mask_sim)\n",
    "                    \n",
    "                \n",
    "                if task_type==\"kitti\":\n",
    "                    vectorized_map=np.vectorize(data_generator_utils.map_values_car)\n",
    "                elif task_type==\"donkey\":\n",
    "                    vectorized_map=np.vectorize(data_generator_utils.map_values_donkey)\n",
    "                real_label_value=created_mask_real\n",
    "\n",
    "                \n",
    "                binary_mask_pix2pix=data_generator_utils.cast_to_int32(binary_mask_pix2pix)\n",
    "                binary_mask_pix2pix=np.squeeze(binary_mask_pix2pix)\n",
    "                binary_mask_real=data_generator_utils.cast_to_int32(binary_mask_real)\n",
    "                binary_mask_real=np.squeeze(binary_mask_real)\n",
    "\n",
    "                created_mask_real=data_generator_utils.cast_to_int32(created_mask_real)\n",
    "                created_mask_real=np.squeeze(created_mask_real)\n",
    "                created_mask_sim=data_generator_utils.cast_to_int32(created_mask_sim)\n",
    "                created_mask_sim=np.squeeze(created_mask_sim)\n",
    "\n",
    "                error_pix2pix_car=abs(binary_mask_pix2pix-binary_mask_real)\n",
    "                error_sim_real=abs(created_mask_sim-created_mask_real)\n",
    "\n",
    "                if (input_domain==\"real\"):\n",
    "\n",
    "                    error_pix2pix_sim_mask_real=abs(created_mask_pix2pix_mask_real-created_mask_sim)\n",
    "                    error_pix2pix_real_mask_real=abs(created_mask_pix2pix_mask_real-created_mask_real)\n",
    "                    \n",
    "                    error_data_pix2pix_mask_real = {\n",
    "                        \"sim_real\": np.sum(error_sim_real) / (width * height),\n",
    "                        \"sim\": np.sum(error_pix2pix_sim_mask_real) / (width * height),\n",
    "                        \"real\": np.sum(error_pix2pix_real_mask_real) / (width * height),\n",
    "                        \"additional\": np.sum(error_pix2pix_car) / (width * height)\n",
    "                    }\n",
    "    \n",
    "                    output_folder = './'+task_type+'/content/output_plots/pix2pix_mask_'+pix2pix_mask_type+'/'+pix2pix_mask_name+'_real_mask_error/'\n",
    "                    os.makedirs(output_folder, exist_ok=True)\n",
    "                    output_path = output_folder + str(dataset_index) + \"_\" + str(i) +\".json\"\n",
    "                    with open(output_path, \"w\") as json_file:\n",
    "                        json.dump(error_data_pix2pix_mask_real, json_file, indent=4)\n",
    "                    \n",
    "                else:\n",
    "                    error_pix2pix_sim_mask_sim=abs(created_mask_pix2pix_mask_sim-created_mask_sim)\n",
    "                    error_pix2pix_real_mask_sim=abs(created_mask_pix2pix_mask_sim-created_mask_real)\n",
    "                    error_data_pix2pix_mask_sim = {\n",
    "                        \"sim_real\": np.sum(error_sim_real) / (width * height),\n",
    "                        \"sim\": np.sum(error_pix2pix_sim_mask_sim) / (width * height),\n",
    "                        \"real\": np.sum(error_pix2pix_real_mask_sim) / (width * height),\n",
    "                        \"additional\": np.sum(error_pix2pix_car) / (width * height)\n",
    "                    }\n",
    "                    output_folder = './'+task_type+'/content/output_plots/pix2pix_mask_'+pix2pix_mask_type+'/'+pix2pix_mask_name+'_sim_mask_error/'\n",
    "                    os.makedirs(output_folder, exist_ok=True)\n",
    "                    output_path = output_folder + str(dataset_index) + \"_\" + str(i) +\".json\"\n",
    "                    with open(output_path, \"w\") as json_file:\n",
    "                        json.dump(error_data_pix2pix_mask_sim, json_file, indent=4)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1133d161-9ba9-4c46-bfa7-5072a97aeac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_type='donkey'\n",
    "output_folder = './'+task_type+'/content/output_plots'\n",
    "plot=False\n",
    "two_classes=False\n",
    "limit=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c1c6329-1a8c-4ad0-b298-2bef17509e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './'+task_type+'/content/datasets/h5_out/raw_image_sim.h5'\n",
    "loaded_dictionary_images_sim=data_generator_utils.load_h5_to_dictionary(file_path)\n",
    "file_path = './'+task_type+'/content/datasets/h5_out/raw_image_real.h5'\n",
    "loaded_dictionary_images_real=data_generator_utils.load_h5_to_dictionary(file_path)\n",
    "file_path = './'+task_type+'/content/datasets/h5_out/semantic_id_list_real.h5'\n",
    "loaded_semantic_id_real=data_generator_utils.load_h5_to_dictionary(file_path)\n",
    "file_path = './'+task_type+'/content/datasets/h5_out/semantic_id_list_sim.h5'\n",
    "loaded_semantic_id_sim=data_generator_utils.load_h5_to_dictionary(file_path)\n",
    "\n",
    "if task_type==\"kitti\":\n",
    "    file_path = './'+task_type+'/content/datasets/h5_out/bounding_boxes_sim.h5'\n",
    "    loaded_bounding_sim=data_generator_utils.load_h5_to_dictionary(file_path)\n",
    "    file_path = './'+task_type+'/content/datasets/h5_out/bounding_boxes_real.h5'\n",
    "    loaded_bounding_real=data_generator_utils.load_h5_to_dictionary(file_path)\n",
    "    height,width=374,1238\n",
    "    road=0\n",
    "    additional_id=3\n",
    "    additional_id_init=13\n",
    "    dataset_index_list_test=[\"0001\",\"0002\",\"0006\",\"0018\",\"0020\"]\n",
    "    pattern = 'tvvttvttnn'\n",
    "    \n",
    "elif task_type==\"donkey\":\n",
    "    height,width=140,320\n",
    "    road=1\n",
    "    additional_id=1\n",
    "    additional_id_init=2\n",
    "    dataset_index_list_test=[\"0001\"]\n",
    "    pattern = 'vvvvvvvvvv'\n",
    "    \n",
    "else:\n",
    "    print(\"Choose a set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2722ada9-b82d-4e38-88b8-ff74a0c34f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAN train and test\n",
      "Dataset 0001\n",
      "Train:  0\n",
      "Test:  196\n",
      "140 320\n"
     ]
    }
   ],
   "source": [
    "train_indexes_gan,test_indexes_gan=data_generator_utils.get_gan_indexes(dataset_index_list_test,loaded_dictionary_images_real,loaded_dictionary_images_sim,pattern)\n",
    "loaded_dictionary_images_real,loaded_dictionary_images_sim,loaded_semantic_id_real,loaded_semantic_id_sim=data_generator_utils.crop_data_dictionaries(task_type,dataset_index_list_test,loaded_dictionary_images_real,loaded_dictionary_images_sim,loaded_semantic_id_real,loaded_semantic_id_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af784d0f-40bf-4b17-b994-b3990a251ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Metal device set to: Apple M2 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-27 16:20:13.173039: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-10-27 16:20:13.173192: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "Some layers from the model checkpoint at nvidia/mit-b0 were not used when initializing TFSegformerForSemanticSegmentation: ['classifier']\n",
      "- This IS expected if you are initializing TFSegformerForSemanticSegmentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFSegformerForSemanticSegmentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFSegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b0 and are newly initialized: ['decode_head']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "checkpoint_file_path = \"./\"+task_type+\"/content/segmentation_checkpoints/Model_weights.hdf5\"\n",
    "segmentation_model=data_generator_utils.load_segmentation_model(checkpoint_file_path,task_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b686163-f7f4-4c83-8786-4b710482874b",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_path_name=\"real\"\n",
    "sim_path_name=\"sim\"\n",
    "save_sim_real_outputs(task_type,segmentation_model,real_path_name,sim_path_name,additional_id,height,width,dataset_index_list_test,test_indexes_gan,loaded_dictionary_images_real,loaded_dictionary_images_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d85621-671e-4bf4-a358-4522bda44ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_paths = ['./'+task_type+'/content/gan_checkpoints/cyclegan_checkpoints/1','./'+task_type+'/content/gan_checkpoints/cyclegan_checkpoints/2','./'+task_type+'/content/gan_checkpoints/cyclegan_checkpoints/3']\n",
    "checkpoint_names=[\"cyclegan_1\",\"cyclegan_2\",\"cyclegan_3\"]\n",
    "\n",
    "for checkpoint_path,checkpoint_name in zip(checkpoint_paths,checkpoint_names):\n",
    "    print(checkpoint_path)\n",
    "    generator_cyclegan = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n",
    "    ckpt_cyclegan = tf.train.Checkpoint(generator_g=generator_cyclegan)\n",
    "    ckpt_manager_cyclegan = tf.train.CheckpointManager(ckpt_cyclegan, checkpoint_path, max_to_keep=5)\n",
    "    if ckpt_manager_cyclegan.latest_checkpoint:\n",
    "        ckpt_cyclegan.restore(ckpt_manager_cyclegan.latest_checkpoint)\n",
    "        print ('Checkpoint restored')\n",
    "    else:\n",
    "        print ('No Checkpoint!!!')\n",
    "    print(height,width)\n",
    "    save_cyclegan_outputs(task_type,segmentation_model,generator_cyclegan,checkpoint_name,additional_id,height,width,dataset_index_list_test,test_indexes_gan,loaded_dictionary_images_real,loaded_dictionary_images_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a980103a-3925-4145-ba89-2a15b1b0a9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_paths = ['./'+task_type+'/content/gan_checkpoints/pix2pix_checkpoints/1','./'+task_type+'/content/gan_checkpoints/cyclegan_checkpoints/2','./'+task_type+'/content/gan_checkpoints/cyclegan_checkpoints/3']\n",
    "checkpoint_names=[\"pix2pix_mask_1\",\"pix2pix_mask_2\",\"pix2pix_mask_3\"]\n",
    "    \n",
    "\n",
    "input_domain=\"sim\"\n",
    "pix2pix_mask_type=\"manual\"\n",
    "\n",
    "for checkpoint_path,checkpoint_name in zip(checkpoint_paths,checkpoint_names):\n",
    "    generator_pix2pix_mask_real = data_generator_utils.Generator()\n",
    "    ckpt_pix2pix_mask_real = tf.train.Checkpoint(\n",
    "                                     generator=generator_pix2pix_mask_real)\n",
    "    ckpt_manager_pix2pix_mask_real = tf.train.CheckpointManager(ckpt_pix2pix_mask_real, checkpoint_path, max_to_keep=5)\n",
    "    if ckpt_manager_pix2pix_mask_real.latest_checkpoint:\n",
    "        ckpt_pix2pix_mask_real.restore(ckpt_manager_pix2pix_mask_real.latest_checkpoint)\n",
    "        print ('Checkpoint restored')\n",
    "    else:\n",
    "        print ('No Checkpoint! Check source path')\n",
    "    save_pix2pix_mask_outputs(task_type,limit,segmentation_model,two_classes,generator_pix2pix_mask_real,checkpoint_name,additional_id,height,width,input_domain,pix2pix_mask_type,dataset_index_list_test, test_indexes_gan, loaded_dictionary_images_real,loaded_dictionary_images_sim,loaded_semantic_id_real,loaded_semantic_id_sim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfEnv4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
